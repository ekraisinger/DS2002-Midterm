{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python to Integrate MongoDB Data into ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 - Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = \"localhost\"\n",
    "ports = {\"mongo\" : 27017, \"mysql\" : 3306}\n",
    "\n",
    "# SQL Server login\n",
    "user_id = \"root\"\n",
    "pwd = \"Lyella227\"\n",
    "\n",
    "# atlas\n",
    "atlas_cluster_name = \"cluster0\"\n",
    "atlas_default_dbname = \"classicmodels\"\n",
    "atlas_user_name = \"emrkraisinger\"\n",
    "atlas_password = \"Lyella227\"\n",
    "\n",
    "conn_str = {\"local\" : f\"mongodb://{host_name}:{ports['mongo']}/\",\n",
    "    \"atlas\" : f\"mongodb+srv://{atlas_user_name}:{atlas_password}@{atlas_cluster_name}.pfsh0cc.mongodb.net/{atlas_default_dbname}\"\n",
    "}\n",
    "\n",
    "src_dbname = \"classicmodels\"\n",
    "dst_dbname = \"classicmodels_dw3\"\n",
    "print(conn_str) # this gives you the conn string\n",
    "\n",
    "#mongodb+srv://emrkraisinger:<password>@cluster0.pfsh0cc.mongodb.net/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_dataframe(user_id, pwd, host_name, db_name, sql_query):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    \n",
    "    '''Invoke the pd.read_sql() function to query the database, and fill a Pandas DataFrame.'''\n",
    "    conn = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(sql_query, conn);\n",
    "    conn.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def get_mongo_dataframe(connect_str, db_name, collection, query):\n",
    "    '''Create a connection to MongoDB'''\n",
    "    client = pymongo.MongoClient(connect_str)\n",
    "    \n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True) \n",
    "    client.close()\n",
    "    return dframe\n",
    "\n",
    "\n",
    "def get_mongo_dataframe_local(user_id, pwd, host_name, port, db_name, collection, query):\n",
    "    '''Create a connection to MongoDB, with or without authentication credentials'''\n",
    "    if user_id and pwd:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db_name)\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn_str = f\"mongodb://{host_name}:{port}/\"\n",
    "        client = pymongo.MongoClient(conn_str)\n",
    "    \n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    client.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "def set_dataframe(user_id, pwd, host_name, db_name, df, table_name, pk_column, db_operation):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the Pandas DataFrame .to_sql( ) function to either create, or append to, a table'''\n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "   \n",
    "        #sqlEngine.execute(f\"ALTER TABLE {table_name} DROP CONSTRANT ({pk_column});\")\n",
    "        #sqlEngine.execute(f\"ADD CONSTRAINT {pk_column} PRIMARY KEY NONCLUSTERED({primary_key});\")\n",
    "        sqlEngine.execute(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\")\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(conn_str[\"atlas\"])\n",
    "db = client[src_dbname]\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'classicdata')\n",
    "\n",
    "json_files = {\"customers\" : 'classicmodels_customers.json',\n",
    "              \"payments\" : 'classicmodels_payments.json',\n",
    "              \"products\" : 'classicmodels_products.json',\n",
    "              \"date\" : 'classicmodels_date.json',\n",
    "             }\n",
    "\n",
    "for file in json_files:\n",
    "    db.drop_collection(file)\n",
    "    json_file = os.path.join(data_dir, json_files[file])\n",
    "    with open(json_file, 'r') as openfile:\n",
    "        json_object = json.load(openfile)\n",
    "        file = db[file]\n",
    "        result = file.insert_many(json_object)\n",
    "        #print(f\"{file} was successfully loaded.\")\n",
    "\n",
    "        \n",
    "client.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 - Create and Populate the New Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Extract Data from the Source MongoDB Collections Into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {}\n",
    "collection = \"customers\"\n",
    "\n",
    "df_customers = get_mongo_dataframe(conn_str['atlas'], src_dbname, collection, query) #connect to atlas (mongodb in cloud). pull info that matches query {} from suppliers collection\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {}\n",
    "collection = \"payments\"\n",
    "\n",
    "df_payments = get_mongo_dataframe(conn_str['atlas'], src_dbname, collection, query) #connect to atlas (mongodb in cloud). pull info that matches query {} from suppliers collection\n",
    "df_payments.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {}\n",
    "collection = \"products\"\n",
    "\n",
    "df_products = get_mongo_dataframe(conn_str['atlas'], src_dbname, collection, query) #connect to atlas (mongodb in cloud). pull info that matches query {} from suppliers collection\n",
    "df_products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {}\n",
    "collection = \"date\"\n",
    "\n",
    "df_date = get_mongo_dataframe(conn_str['atlas'], src_dbname, collection, query) #connect to atlas (mongodb in cloud). pull info that matches query {} from suppliers collection\n",
    "df_date.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Perform Transformations to Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['salesRepEmployeeNumber','creditLimit']\n",
    "df_customers.drop(drop_cols, axis=1, inplace=True)\n",
    "df_customers.insert(0, \"customer_key\", range(1, df_customers.shape[0]+1))\n",
    "\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a primary key to payments table\n",
    "df_payments.insert(0, \"payments_key\", range(1, df_payments.shape[0]+1))\n",
    "df_payments.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['productScale','productVendor','productDescription']\n",
    "df_products.drop(drop_cols, axis=1, inplace=True)\n",
    "df_products.insert(0, \"products_key\", range(1, df_products.shape[0]+1))\n",
    "\n",
    "df_products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['date_name','date_name_us','date_name_eu', 'date_name_eu',\n",
    "            'day_of_week','day_name_of_week','date_name_eu','day_of_month',\n",
    "            'day_of_year','weekday_weekend','is_last_day_of_month','day_of_month',\n",
    "            'calendar_year_month','calendar_year_qtr','fiscal_month_of_year',\n",
    "            'fiscal_year_month','fiscal_year_qtr','month_of_year','calendar_quarter','calendar_year','week_of_year','month_name']\n",
    "df_date.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "df_date.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Load transformed dataframes into data warehouse by creating new tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_customers\n",
    "table_name = 'dim_customers'\n",
    "primary_key = 'customer_key'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, primary_key, db_operation) # connect to mySQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_payments\n",
    "table_name = 'dim_payments'\n",
    "primary_key = 'payments_key'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_products\n",
    "table_name = 'dim_products'\n",
    "primary_key = 'products_key'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_date\n",
    "table_name = 'dim_date'\n",
    "primary_key = 'date_key'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 - Validate that new dimension tables were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_customers = \"SELECT * FROM classicmodels_dw3.dim_customers;\"\n",
    "df_dim_customers = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_customers)\n",
    "df_dim_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_payments = \"SELECT * FROM classicmodels_dw3.dim_payments;\"\n",
    "df_dim_payments = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_payments)\n",
    "df_dim_payments.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_products = \"SELECT * FROM classicmodels_dw3.dim_products;\"\n",
    "df_dim_products = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_products)\n",
    "df_dim_products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_date = \"SELECT * FROM classicmodels_dw3.dim_date;\"\n",
    "df_dim_date= get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_date)\n",
    "df_dim_date.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 - Create and populate new fact tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Extract orders/orderdetails tables from MySQL into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE FACT ORDER TABLE FROM ORDERS\n",
    "query = {} \n",
    "\n",
    "sql_fact_orders = \"SELECT * FROM classicmodels.orders;\"\n",
    "df_fact_orders = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_fact_orders)\n",
    "df_fact_orders.insert(0, \"fact_order_key\", range(1, df_fact_orders.shape[0]+1))\n",
    "\n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT ORDERDETAILS\n",
    "query = {} \n",
    "\n",
    "sql_order_details = \"SELECT * FROM classicmodels.orderdetails;\"\n",
    "df_order_details = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_order_details)\n",
    "df_order_details.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 - Fix date/time columns to properly merge dim_date into fact orders table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date['full_date'] =  df_dim_date['full_date'].astype('datetime64[ns]')\n",
    "\n",
    "df_dim_date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Transform dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 - Merge tables to add primary keys of the dimension tables to fact order table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE ORDERS & ORDER DETAILS\n",
    "df_fact_orders_table = pd.merge(df_fact_orders, df_order_details, on='orderNumber', how='inner') # inner means only vals that match (not NaN)\n",
    "\n",
    "df_fact_orders_table.rename(columns={\"orderNumber\":\"order_key\"}, inplace=True)\n",
    "df_fact_orders_table.drop(['comments','orderLineNumber','requiredDate','shippedDate'], axis=1, inplace=True)\n",
    "\n",
    "# change fact/orders data type to merge properly with date dimension table\n",
    "df_fact_orders_table['order_date'] =  df_fact_orders_table['orderDate'].astype('datetime64[ns]')\n",
    "\n",
    "df_fact_orders_table.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE FACT ORDERS & CUSTOMERS\n",
    "df_fact_orders_table = pd.merge(df_fact_orders_table, df_customers, on='customerNumber', how='inner') # inner means only vals that match (not NaN)\n",
    "df_fact_orders_table.drop(['customerName','contactLastName','contactFirstName','phone','addressLine1','addressLine2','city', 'state', 'postalCode', 'country'], axis=1, inplace=True)\n",
    "\n",
    "df_fact_orders_table.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE FACT ORDERS & PAYMENTS\n",
    "df_fact_orders_table = pd.merge(df_fact_orders_table, df_payments, on='customerNumber', how='inner') # inner means only vals that match (not NaN)\n",
    "df_fact_orders_table.drop(['paymentDate', 'checkNumber', 'amount','quantityOrdered','priceEach'], axis=1, inplace=True)\n",
    "\n",
    "df_fact_orders_table.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE FACT ORDERS & PRODUCTS\n",
    "df_fact_orders_table = pd.merge(df_fact_orders_table, df_products, on='productCode', how='inner') # inner means only vals that match (not NaN)\n",
    "df_fact_orders_table.rename(columns={\"productCode\":\"product_key\"}, inplace=True)\n",
    "df_fact_orders_table.drop(['productName', 'productLine', 'buyPrice','MSRP'], axis=1, inplace=True)\n",
    "\n",
    "df_fact_orders_table.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE FACT ORDERS AND DATE DIMENSION\n",
    "df_fact_orders_table = pd.merge(df_fact_orders_table, df_dim_date, left_on='order_date', right_on='full_date', how='left') \n",
    "df_fact_orders_table = df_fact_orders_table.drop(labels=['orderDate', 'full_date'], axis=1)\n",
    "\n",
    "df_fact_orders_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 - Clean up df_fact_orders by reordering columns and dropping duplicate values if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order columns \n",
    "ordered_columns = ['fact_order_key','order_key','product_key','customer_key','payments_key','products_key',\n",
    "                   'date_key','status']\n",
    "df_fact_orders = df_fact_orders_table[ordered_columns]\n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df_fact_orders = df_fact_orders.sort_values(by=['fact_order_key'], axis=0, ascending=True)\n",
    "df_fact_orders = df_fact_orders.drop_duplicates(subset=['fact_order_key'], keep='first', inplace=False, ignore_index=False)\n",
    "\n",
    "df_fact_orders.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Load transformed fact orders table into classicmodels_dw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_fact_orders\n",
    "table_name = 'fact_orders_table'\n",
    "pk_column = 'fact_order_key'\n",
    "db_operation = \"update\"\n",
    "\n",
    "set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, pk_column, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 - Validate that new fact table was loaded correctly into MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_fact_orders = \"SELECT * FROM classicmodels_dw3.fact_orders_table;\"\n",
    "df_fact_orders_verification = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_fact_orders)\n",
    "df_fact_orders_verification.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Create 3 select statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average payment made\n",
    "sql_test1 = \"\"\" \n",
    "   SELECT AVG(`amount`) FROM `classicmodels_dw3`.`dim_payments`;\"\"\".format(dst_dbname)\n",
    "df_test1 = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_test1)\n",
    "df_test1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of unique primary fact order keys\n",
    "sql_test2 = \"\"\" \n",
    "   SELECT COUNT(`fact_order_key`) FROM `classicmodels_dw3`.`fact_orders_table`;\"\"\".format(dst_dbname)\n",
    "df_test2 = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_test2)\n",
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum MSRP (manufacturer's suggested retail price) for a product\n",
    "sql_test3 = \"\"\" \n",
    "   SELECT MAX(`MSRP`) FROM `classicmodels_dw3`.`dim_products`;\"\"\".format(dst_dbname)\n",
    "df_test3 = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_test3)\n",
    "df_test3.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1638293095d7d0dc004ab20ca1d1f99f450d9200987beb26b5e0ce2ecff614e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
